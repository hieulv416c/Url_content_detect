{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10997075,
          "sourceType": "datasetVersion",
          "datasetId": 6845595
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "BERT+Machinelearning",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieulv416c/Url_content_detect/blob/main/BERT%2BMachinelearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "lehieu12_cleaned_data_url68k_path = kagglehub.dataset_download('lehieu12/cleaned-data-url68k')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "8Qg3L8yxmP05"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Import the libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "!pip install transformers tensorflow numpy pandas scikit-learn nltk\n",
        "!pip install underthesea  # N·∫øu d·ªØ li·ªáu l√† ti·∫øng Vi·ªát\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Load data\n",
        "file_path = '/kaggle/input/cleaned-data-url68k/content_cleaned.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "\n",
        "df[\"Content\"] = df[\"Content\"].astype(str)\n",
        "\n",
        "!pip install catboost\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler  # Th√™m chu·∫©n h√≥a\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# üìå T·∫£i m√¥ h√¨nh BERT t·ª´ Hugging Face\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# üìå H√†m chuy·ªÉn vƒÉn b·∫£n th√†nh BERT embedding (s·ª≠ d·ª•ng vector CLS)\n",
        "def bert_embedding(texts, batch_size=8):\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i : i + batch_size]\n",
        "\n",
        "        # Token h√≥a v√† padding\n",
        "        encoded_inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=50, return_tensors=\"tf\")\n",
        "\n",
        "        # Ch·∫°y qua m√¥ h√¨nh BERT\n",
        "        outputs = bert_model(**encoded_inputs)\n",
        "\n",
        "        # L·∫•y vector CLS (token ƒë·∫ßu ti√™n c·ªßa last_hidden_state)\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "        embeddings.append(batch_embeddings)\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "\n",
        "\n",
        "# Chia t·∫≠p train/test\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df[\"Content\"].tolist(), df[\"result\"].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh embedding v·ªõi batch nh·ªè\n",
        "X_train = bert_embedding(train_texts, batch_size=8)\n",
        "X_test = bert_embedding(test_texts, batch_size=8)\n",
        "y_train = np.array(train_labels, dtype=np.int64)\n",
        "y_test = np.array(test_labels, dtype=np.int64)\n",
        "\n",
        "\n",
        "\n",
        "# üìå Danh s√°ch m√¥ h√¨nh ML\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
        "    \"SVM\": SVC(kernel=\"linear\", random_state=42),\n",
        "}\n",
        "\n",
        "# # üìå Danh s√°ch c√°c m√¥ h√¨nh C·∫¶N chu·∫©n h√≥a\n",
        "# models_need_scaling = [\"SVM\", \"Logistic Regression\", \"KNN\", \"MLP Classifier\"]\n",
        "\n",
        "# # √Åp d·ª•ng StandardScaler cho nh·ªØng m√¥ h√¨nh c·∫ßn\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# üèÜ Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nüöÄ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh: {name}...\")\n",
        "\n",
        "    # # Ki·ªÉm tra n·∫øu m√¥ h√¨nh c·∫ßn chu·∫©n h√≥a\n",
        "    # if name in models_need_scaling:\n",
        "    #     model.fit(X_train_scaled, y_train)\n",
        "    #     y_pred = model.predict(X_test_scaled)\n",
        "    # else:\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"üéØ ƒê·ªô ch√≠nh x√°c: {accuracy:.4f}\")\n",
        "    print(\"üìä B√°o c√°o ph√¢n lo·∫°i:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Kh√¥ng Phishing\", \"Phishing\"], yticklabels=[\"Kh√¥ng Phishing\", \"Phishing\"])\n",
        "    plt.xlabel(\"D·ª± ƒëo√°n\")\n",
        "    plt.ylabel(\"Th·ª±c t·∫ø\")\n",
        "    plt.title(f\"üîç Ma tr·∫≠n nh·∫ßm l·∫´n - {name}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"üéâ Qu√° tr√¨nh hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh ho√†n t·∫•t!\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T10:09:13.86854Z",
          "iopub.execute_input": "2025-03-12T10:09:13.868845Z",
          "iopub.status.idle": "2025-03-12T12:09:53.848826Z",
          "shell.execute_reply.started": "2025-03-12T10:09:13.868796Z",
          "shell.execute_reply": "2025-03-12T12:09:53.847932Z"
        },
        "id": "RF857LhImP06"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}