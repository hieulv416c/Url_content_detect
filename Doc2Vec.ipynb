{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieulv416c/Url_content_detect/blob/main/Doc2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLly6Qawv0h3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MUPj_o6Jl98",
        "outputId": "a5f1eb9f-a2ff-4806-8c91-a6cc8df86cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIosWigtNL_o"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duVglXzELoX3"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdN6N2brO68o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d318c7cd-d365-46ba-ad7c-4339e4113bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_WVdwGQLpSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40838b09-c6b9-4337-c0f7-936744075453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.1.8)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->underthesea) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (3.5.0)\n",
            "Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.11 underthesea-6.8.4 underthesea-core-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers tensorflow numpy pandas scikit-learn nltk\n",
        "!pip install underthesea  # Nếu dữ liệu là tiếng Việt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2sFwpqTnaoR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wkyjkz-r65Wv",
        "outputId": "e1c4ae38-247e-46bc-8eae-82dc135414c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW1WC43x7GSp",
        "outputId": "f12aa126-a271-4c5d-a083-6f68e3ab18f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       File Name                                              Title  \\\n",
            "0  10000847.html  Wells Fargo – Banking, Credit Cards, Loans, Mo...   \n",
            "1  10000855.html  La Banque Postale - Banque et Assurance en lig...   \n",
            "2  10000856.html                                   LCL - Mon espace   \n",
            "3  10000946.html                                           No title   \n",
            "4  10000976.html                                           Facebook   \n",
            "\n",
            "                                             Content  \\\n",
            "0  well fargo banking credit card loan mortgage s...   \n",
            "1  la banque postale banque et assurance en ligne...   \n",
            "2  lcl mon espace votre identifiant mémoriser mon...   \n",
            "3  ov5qtm_ydkxfcuwsmak29bb94o4mxy5n3mpm62bkahmjda...   \n",
            "4  facebook messenger albin berisha albin shared ...   \n",
            "\n",
            "                                                 url  result  \\\n",
            "0                              http://serg.co.il/gy/     1.0   \n",
            "1  http://spectra.brydierobinson.com/wp-admin/POS...     1.0   \n",
            "2  http://thehillbookhouse.com/DSP2-Authentificat...     1.0   \n",
            "3                  https://amznb.jp.caxnews1.com.cn/     1.0   \n",
            "4              https://fbpages-case10004915.web.app/     1.0   \n",
            "\n",
            "          created_date                                  Tokenized_Content  \n",
            "0  2021-10-31 22:55:18  ['well', 'fargo', 'banking', 'credit', 'card',...  \n",
            "1  2021-10-31 22:55:19  ['la', 'banque', 'postale', 'banque', 'et', 'a...  \n",
            "2  2021-10-31 22:55:19  ['lcl', 'mon', 'espace', 'votre', 'identifiant...  \n",
            "3  2021-10-31 22:55:23  ['ov5qtm_ydkxfcuwsmak29bb94o4mxy5n3mpm62bkahmj...  \n",
            "4  2021-10-31 22:55:25  ['facebook', 'messenger', 'albin', 'berisha', ...  \n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "file_path = '/content/drive/Shared drives/NCKH/DATA/content_cleaned.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtY7-jK3KxKh"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = df.dropna()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaIIhzg3epdw"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCSHmoPpqOJP"
      },
      "outputs": [],
      "source": [
        "# Nếu chưa tải stopwords, hãy tải\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))  # Nếu tiếng Anh\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()  # Chuyển thành chữ thường\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Loại bỏ HTML tags\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Loại bỏ ký tự đặc biệt\n",
        "    text = re.sub(r'\\d+', '', text)  # Loại bỏ số\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Loại bỏ stopwords\n",
        "    return text\n",
        "\n",
        "# Áp dụng tiền xử lý\n",
        "df['Content'] = df['Content'].apply(clean_text)\n",
        "\n",
        "# Kiểm tra kết quả\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4TH5xHrXZzb"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZrYQ9IJXgZh"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# num_rows, num_cols = df.shape\n",
        "# print(f\"Tập dữ liệu có {num_rows} dòng và {num_cols} cột.\")\n",
        "\n",
        "# # 2. Phân bố nhãn result\n",
        "# label_distribution = df['result'].value_counts()\n",
        "# print(\"Phân bố nhãn result:\")\n",
        "# print(label_distribution)\n",
        "\n",
        "# # 3. Ví dụ vài dòng từ cột Content\n",
        "# print(\"\\nVí dụ vài dòng từ cột Content:\")\n",
        "# print(df['Content'].sample(5, random_state=42))  # Lấy ngẫu nhiên 5 dòng\n",
        "\n",
        "# # 4. Độ dài trung bình của Content\n",
        "# df['content_length'] = df['Content'].astype(str).apply(len)\n",
        "# print(\"\\nThống kê độ dài Content:\")\n",
        "# print(df['content_length'].describe())\n",
        "\n",
        "# # 5. Kiểm tra dữ liệu bị thiếu\n",
        "# missing_data = df.isnull().sum()\n",
        "# print(\"\\nSố lượng giá trị bị thiếu trong mỗi cột:\")\n",
        "# print(missing_data[missing_data > 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtYIefzCZfZt"
      },
      "outputs": [],
      "source": [
        "# Loại bỏ các dòng có Content rỗng\n",
        "df = df[df['Content'].astype(str).str.strip() != \"\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcpx2om9Zh9H"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA2KG5ZuNOn4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Chuẩn bị dữ liệu cho Doc2Vec\n",
        "# tagged_data = [TaggedDocument(words=words, tags=[str(i)]) for i, words in enumerate(df[\"Content\"])]\n",
        "\n",
        "# # Huấn luyện mô hình Doc2Vec\n",
        "# doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=2, workers=4, epochs=20)\n",
        "\n",
        "\n",
        "# # Chuyển văn bản thành vector trung bình của các từ trong Word2Vec\n",
        "# def get_sentence_vector(words, model):\n",
        "#     vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "#     if len(vectors) > 0:\n",
        "#         return np.mean(vectors, axis=0)\n",
        "#     else:\n",
        "#         return np.zeros(model.vector_size)\n",
        "\n",
        "# df[\"word2vec_vectorized\"] = df[\"Content\"].apply(lambda words: get_sentence_vector(words, word2vec_model))\n",
        "# df[\"doc2vec_vectorized\"] = [doc2vec_model.dv[str(i)] for i in range(len(df))]\n",
        "\n",
        "# # Tạo ma trận đặc trưng từ Doc2Vec\n",
        "# X = np.vstack(df[\"doc2vec_vectorized\"].values)\n",
        "# y = df[\"result\"].values\n",
        "\n",
        "# # Chia tập train/test\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Khởi tạo và huấn luyện mô hình\n",
        "# models = {\n",
        "#     \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "#     \"SVM\": SVC(kernel='linear', random_state=42),\n",
        "#     \"Logistic Regression\": LogisticRegression(random_state=42)\n",
        "# }\n",
        "\n",
        "# for name, model in models.items():\n",
        "#     model.fit(X_train, y_train)\n",
        "#     y_pred = model.predict(X_test)\n",
        "#     print(f\"\\n{name} Model Performance:\")\n",
        "#     print(accuracy_score(y_test, y_pred))\n",
        "#     print(classification_report(y_test, y_pred))\n",
        "\n",
        "# print(\"Quá trình huấn luyện và đánh giá mô hình với Doc2Vec hoàn tất!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import pickle\n",
        "\n",
        "# Kiểm tra và xử lý dữ liệu đầu vào\n",
        "df[\"Content\"] = df[\"Content\"].astype(str).apply(lambda x: x.split())\n",
        "# Chuẩn bị dữ liệu cho Doc2Vec\n",
        "tagged_data = [TaggedDocument(words=words, tags=[str(i)]) for i, words in enumerate(df[\"Content\"])]\n",
        "\n",
        "# Huấn luyện mô hình Doc2Vec\n",
        "doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=2, workers=4, epochs=20)\n",
        "\n",
        "# Lưu mô hình Doc2Vec\n",
        "doc2vec_model.save(\"doc2vec_model.model\")\n",
        "\n",
        "# Chuyển văn bản thành vector Doc2Vec\n",
        "df[\"doc2vec_vectorized\"] = [doc2vec_model.dv[str(i)] for i in range(len(df))]\n",
        "\n",
        "# Lưu dữ liệu đã vector hóa\n",
        "df.to_csv(\"doc2vec_vectorized_data.csv\", index=False)\n",
        "\n",
        "print(\"Dữ liệu đã được vector hóa và lưu thành công!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA6uRqqp6vd4",
        "outputId": "af95e0f7-091e-41ea-bfc6-8e981c125928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dữ liệu đã được vector hóa và lưu thành công!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOW+7LgnMIa4Uv+bPOvIkRw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}